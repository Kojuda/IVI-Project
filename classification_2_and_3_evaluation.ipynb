{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Ce script permet de créer des sous-échantillons des classifications 2 et 3 avec une pondération\n",
    "suivant la règle log(1/sum(freq(assignation))) qui permet d'obtenir les assignations d'espèces rares\n",
    "dans une quantité plus raisonnable tout en rendant le tirage d'assignation très fréquentes proportionnel\n",
    "à leur apparition. Cela permet d'évaluer de manière plus complète les cas particulier de la classification\n",
    "avec un petit échantillon labelisé à la main. Le script évalue les statistiques d'évaluation une fois que \n",
    "les tables sont remplies à la main pour mettre le score de classement de chaque annonce.\n",
    "\"\"\"\n",
    "\n",
    "size_sample=100\n",
    "# Create a SQL connection to our SQLite database\n",
    "con = sqlite3.connect(\"DATABASES/project.db\")\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data into a DataFrame\n",
    "class_2_df = pd.read_sql_query(\"SELECT * from classification_2_matching_ads\", con)\n",
    "class_3_df = pd.read_sql_query(\"SELECT * from classification_3_matching_ads\", con)\n",
    "\n",
    "class_2_df.index = class_2_df.id\n",
    "class_2_df.drop(columns=[\"id\"], inplace=True)\n",
    "class_3_df.index = class_3_df.id\n",
    "class_3_df.drop(columns=[\"id\"], inplace=True)\n"
   ]
  },
  {
   "source": [
    "# Balanced sampling through species"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Weighted the sampling with the logarithm of the inverse frequency of the a assignation. \n",
    "\n",
    "def balanced_sample(df, size_sample) :\n",
    "    \"\"\"Create a balanced sample according to the frequency of the species. We take the \n",
    "    frequency of an assignation and we weight this assignation with the logarithm of the inverse of the \n",
    "    frequency\"\"\"\n",
    "    df=df.copy()\n",
    "    list_class=[]\n",
    "    df[\"ids_matching\"]\\\n",
    "            .apply(lambda x : list_class.extend([int(_) for _ in x.split(\";\")]))\n",
    "    freq_class=Counter(list_class)\n",
    "\n",
    "    df[\"weights\"]=df[\"ids_matching\"]\\\n",
    "            .apply(lambda x :  math.log(1/(sum([freq_class[int(_)]/len(list_class) for _ in x.split(\";\")])))) #take the inverse of the frequencies' sum\n",
    "\n",
    "    #Sample according to the weight\n",
    "    df=df.sample(size_sample, weights=\"weights\")\n",
    "\n",
    "    #Add a verification column\n",
    "    nan_ar = np.empty(size_sample)\n",
    "    nan_ar[:]=np.nan\n",
    "    df[\"verification\"]=nan_ar\n",
    "\n",
    "    return df\n",
    "balanced_s_class2=balanced_sample(class_2_df, size_sample)\n",
    "balanced_s_class3=balanced_sample(class_3_df, size_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write them into the SQL database\n",
    "\n",
    "try:\n",
    "    balanced_s_class2.to_sql(\"classification_2_eval\", con, if_exists=\"fail\")#fail#replace\n",
    "    balanced_s_class3.to_sql(\"classification_3_eval\", con, if_exists=\"fail\")\n",
    "except:\n",
    "    print(\"Samplings already exist\")"
   ]
  },
  {
   "source": [
    "# Rates computation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_2 = pd.read_sql_query(\"SELECT * from classification_2_eval\", con)\n",
    "filled_3 = pd.read_sql_query(\"SELECT * from classification_3_eval\", con)\n",
    "\n",
    "def print_stat(df) :\n",
    "    tp=df.query(\"ids_matching != '-1' & ids_matching != '-2'\")[\"verification\"].astype(int).sum()\n",
    "    tn=df.query(\"ids_matching == '-1' | ids_matching == '-2'\")[\"verification\"].astype(int).sum()\n",
    "    fp=len(df.query(\"ids_matching != '-1' & ids_matching != '-2'\"))-tp\n",
    "    fn=len(df.query(\"ids_matching == '-1' | ids_matching == '-2'\"))-tn\n",
    "\n",
    "    sensitivity=tp/(tp+fn)\n",
    "    specificity=tn/(tn+fp)\n",
    "    precision=tp/(tp+fp)\n",
    "    tpr=tp/(tp+fn)\n",
    "    fpr=fp/(fp+tn)\n",
    "    tnr=tn/(tn+fp)\n",
    "    fnr=fn/(fn+tp)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    \\tSensibilité\\t\\t | {sensitivity:.2f}\\n\n",
    "    (Taux vrai positif)\\n\n",
    "    \\tSpécificité\\t\\t | {specificity:.2f}\\n\n",
    "    (Taux vrai négatif)\\n\n",
    "    \\tPrécision\\t\\t | {precision:.2f}\\n\n",
    "    \\tTaux faux positif\\t | {fpr:.2f}\\n\n",
    "    \\tTaux faux négatif\\t | {fnr:.2f}\\n\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Classification 2 :\\n\")\n",
    "print_stat(filled_2)\n",
    "print(\"Classification 3 :\\n\")\n",
    "print_stat(filled_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to close the connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}